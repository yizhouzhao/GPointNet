{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_tensorboard = True\n",
    "exp_name = \"Oct_26_school\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "print(torch.version.cuda)\n",
    "\n",
    "from torch.autograd import Variable\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if use_tensorboard:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    writer = SummaryWriter(\"runs/\" + exp_name)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.util_torch import *\n",
    "\n",
    "from new.configs import *\n",
    "from new.utils import * \n",
    "from new.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = parse_config()\n",
    "\n",
    "if use_tensorboard:\n",
    "    writer.add_hparams(hparam_dict = vars(opt),metric_dict = {})\n",
    "\n",
    "opt.batch_size = batch_size * 3\n",
    "opt.swap_axis = True\n",
    "if len(opt.checkpoint_path) == 0:\n",
    "    opt.checkpoint_path = None \n",
    "opt.device = \"cuda:%s\" % opt.cuda if opt.cuda!=\"\" else \"cpu\"\n",
    "opt.shuffle = not opt.warm_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.category = \"modelnet10\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = PointCloudDataSet(opt)\n",
    "data_collator = PointCloudDataCollator(opt)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=opt.batch_size, drop_last=opt.drop_last, \n",
    "    shuffle=opt.shuffle, collate_fn = data_collator, num_workers=torch.cuda.device_count() * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# test_dataset = torch.utils.data.TensorDataset(np.load(\"data/%s_test.npy\" % opt.category))\n",
    "# test_loader = torch.utils.data.DataLoader(train_data, batch_size=opt.batch_size, drop_last=opt.drop_last, \n",
    "#     shuffle=False, collate_fn = data_collator, num_workers=torch.cuda.device_count() * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NetWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_gpu(args.device)\n",
    "set_cuda(deterministic=gpu_deterministic)\n",
    "set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optE = torch.optim.Adam(net.netE.parameters(), lr=e_lr, weight_decay=e_decay, betas=(e_beta1, e_beta2))\n",
    "optG = torch.optim.Adam(net.netG.parameters(), lr=g_lr, weight_decay=g_decay, betas=(g_beta1, g_beta2))\n",
    "\n",
    "lr_scheduleE = torch.optim.lr_scheduler.ExponentialLR(optE, e_gamma)\n",
    "lr_scheduleG = torch.optim.lr_scheduler.ExponentialLR(optG, g_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test reference\n",
    "# ref_pcs = np.load(\"data/%s_test.npy\" % opt.category)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "for epoch in tqdm(range(100)):\n",
    "#     if epoch > 0:\n",
    "#         break\n",
    "    # Train phase\n",
    "    net.train()\n",
    "    for c, x in enumerate(train_loader):\n",
    "        total_step += 1\n",
    "        batch_num = x.shape[0]\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.to(\"cuda\")\n",
    "\n",
    "        # Initialize chains\n",
    "        z_g_0 = sample_p_0(n = batch_num,sig=e_init_sig, device = x.device)\n",
    "        z_e_0 = sample_p_0(n = batch_num,sig=e_init_sig, device = x.device)\n",
    "\n",
    "#         print(\"shape log\")\n",
    "#         print(x.shape)\n",
    "#         print(z_g_0.shape)\n",
    "#         print(z_e_0.shape)\n",
    "\n",
    "        # Langevin posterior and prior\n",
    "        z_g_k = net(Variable(z_g_0), x, prior=False)\n",
    "        z_e_k = net(Variable(z_e_0), prior=True)\n",
    "        \n",
    "        # print(\"z_g_k\", z_g_k)\n",
    "        # print(\"z_e_k shape\", z_e_k.shape)\n",
    "        \n",
    "        # Learn generator\n",
    "        optG.zero_grad()\n",
    "        x_hat = net.netG(z_g_k.detach())\n",
    "        \n",
    "        #print(\"x_hat shape\", x_hat.shape)\n",
    "        \n",
    "        loss_g = net.loss_fun(x_hat.transpose(1,2).contiguous(), x.transpose(1,2).contiguous())\n",
    "        loss_g.backward()\n",
    "        optG.step()\n",
    "\n",
    "        # Learn prior EBM\n",
    "        optE.zero_grad()\n",
    "        en_neg = energy(net.netE(z_e_k.detach())).mean() # TODO(nijkamp): why mean() here and in Langevin sum() over energy? constant is absorbed into Adam adaptive lr\n",
    "        en_pos = energy(net.netE(z_g_k.detach())).mean()\n",
    "        loss_e = en_pos - en_neg\n",
    "        loss_e.backward()\n",
    "        # grad_norm_e = get_grad_norm(net.netE.parameters())\n",
    "        # if args.e_is_grad_clamp:\n",
    "        #    torch.nn.utils.clip_grad_norm_(net.netE.parameters(), args.e_max_norm)\n",
    "        optE.step()\n",
    "        \n",
    "        # break\n",
    "        \n",
    "        # Printout\n",
    "        if total_step % 15 == 0:\n",
    "            with torch.no_grad():\n",
    "                x_0 = net.netG(z_e_0)\n",
    "                x_k = net.netG(z_e_k)\n",
    "\n",
    "                en_neg_2 = energy(net.netE(z_e_k)).mean()\n",
    "                en_pos_2 = energy(net.netE(z_g_k)).mean()\n",
    "\n",
    "                prior_moments = '[{:8.2f}, {:8.2f}, {:8.2f}]'.format(z_e_k.mean(), z_e_k.std(), z_e_k.abs().max())\n",
    "                posterior_moments = '[{:8.2f}, {:8.2f}, {:8.2f}]'.format(z_g_k.mean(), z_g_k.std(), z_g_k.abs().max())\n",
    "                \n",
    "                \n",
    "                if use_tensorboard:\n",
    "                    writer.add_scalar('loss/loss_g',loss_g, total_step)\n",
    "                    writer.add_scalar('loss/loss_e',loss_e, total_step)\n",
    "\n",
    "                    writer.add_scalars('energy/en_pos', {'pos_1':en_pos,\n",
    "                                        'pose_2':en_pos_2,\n",
    "                                        'diff': en_pos_2 - en_pos}, total_step)\n",
    "                    writer.add_scalars('energy/en_neg', {'pos_1':en_neg,\n",
    "                                        'pose_2':en_neg_2,\n",
    "                                        'diff': en_neg_2 - en_neg}, total_step)\n",
    "\n",
    "                    writer.add_scalar('value/|z_g_0|',z_g_0.view(batch_num, -1).norm(dim=1).mean(), total_step)\n",
    "                    writer.add_scalar('value/|z_g_k|',z_g_k.view(batch_num, -1).norm(dim=1).mean(), total_step)\n",
    "                    writer.add_scalar('value/|z_e_0|',z_e_0.view(batch_num, -1).norm(dim=1).mean(), total_step)\n",
    "                    writer.add_scalar('value/|z_e_k|',z_e_k.view(batch_num, -1).norm(dim=1).mean(), total_step)\n",
    "\n",
    "                    writer.add_scalar('disp/z_e_disp',(z_e_k-z_e_0).view(batch_num, -1).norm(dim=1).mean(), total_step)\n",
    "                    writer.add_scalar('disp/z_g_disp',(z_g_k-z_g_0).view(batch_num, -1).norm(dim=1).mean(), total_step)\n",
    "                    writer.add_scalar('disp/x_e_disp',(x_k-x_0).view(batch_num, -1).norm(dim=1).mean(), total_step)\n",
    "\n",
    "                    writer.add_scalars('moment/prior_moments', {'mean':z_e_k.mean(),\n",
    "                                        'std':z_e_k.std(),\n",
    "                                        'max abs': z_e_k.abs().max()}, total_step)\n",
    "                    writer.add_scalars('moment/posterior_moments', {'mean':z_g_k.mean(),\n",
    "                                        'std':z_g_k.std(),\n",
    "                                        'max abs': z_g_k.abs().max()}, total_step)\n",
    "                else:\n",
    "                    print(\n",
    "                        '{} {}/{} {}/{} \\n'.format(0, epoch, n_epochs, total_step, len(train_loader)) +\n",
    "                        'loss_g={:8.5f}, \\n'.format(loss_g) +\n",
    "                        'loss_e={:8.5f}, \\n'.format(loss_e) +\n",
    "                        'en_pos=[{:9.5f}, {:9.5f}, {:9.5f}], \\n'.format(en_pos, en_pos_2, en_pos_2-en_pos) +\n",
    "                        'en_neg=[{:9.5f}, {:9.5f}, {:9.5f}], \\n'.format(en_neg, en_neg_2, en_neg_2-en_neg) +\n",
    "                        '|z_g_0|={:6.3f}, \\n'.format(z_g_0.view(batch_num, -1).norm(dim=1).mean()) +\n",
    "                        '|z_g_k|={:6.3f}, \\n'.format(z_g_k.view(batch_num, -1).norm(dim=1).mean()) +\n",
    "                        '|z_e_0|={:6.3f}, \\n'.format(z_e_0.view(batch_num, -1).norm(dim=1).mean()) +\n",
    "                        '|z_e_k|={:6.3f}, \\n'.format(z_e_k.view(batch_num, -1).norm(dim=1).mean()) +\n",
    "                        'z_e_disp={:6.3f}, \\n'.format((z_e_k-z_e_0).view(batch_num, -1).norm(dim=1).mean()) +\n",
    "                        'z_g_disp={:6.3f}, \\n'.format((z_g_k-z_g_0).view(batch_num, -1).norm(dim=1).mean()) +\n",
    "                        'x_e_disp={:6.3f}, \\n'.format((x_k-x_0).view(batch_num, -1).norm(dim=1).mean()) +\n",
    "                        'prior_moments={}, \\n'.format(prior_moments) +\n",
    "                        'posterior_moments={}, \\n'.format(posterior_moments) +\n",
    "                        #'fid={:8.2f}, '.format(fid) +\n",
    "                        #'fid_best={:8.2f}'.format(fid_best)\n",
    "                        \"\\n\\n\\n ---------------------\"\n",
    "                    )\n",
    "                    \n",
    "    # Eval phase\n",
    "    \n",
    "    net.eval()\n",
    "    syn_pcs = net.sample_x(n=16)\n",
    "#     res = quantitative_analysis(syn_pcs.data.numpy(), ref_pcs, 16, full=False)\n",
    "#     if use_tensorboard:\n",
    "#         writer.add_scalar('test_record/jsd', res['jsd'], epoch)\n",
    "#         writer.add_scalar('test_record/mmd-CD', res['mmd-CD'], epoch)\n",
    "#         writer.add_scalar('test_record/mmd-EMD', res['mmd-EMD'], epoch)\n",
    "#         writer.add_scalar('test_record/cov-CD', res['cov-CD'], epoch)\n",
    "#         writer.add_scalar('test_record/cov-EMD', res['cov-EMD'], epoch)\n",
    "#     else:\n",
    "#         print(\"epoch {} test record {}\".format(epoch ,res))\n",
    "        \n",
    "    show_point_clouds(syn_pcs)\n",
    "    print(epoch)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, x in enumerate(train_loader):\n",
    "    print(c)\n",
    "    show_point_clouds(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_point_clouds(x.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_point_clouds(x_hat.cpu().data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net.state_dict(),\"runs/Oct23_11-51-39_yizhou-Z370-AORUS-Gaming-5/model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_pcs = net.sample_x(n=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "syn_pcs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = quantitative_analysis(syn_pcs.data.numpy(), ref_pcs, 16, full=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
