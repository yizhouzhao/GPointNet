{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_langevine = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import logging\n",
    "import random\n",
    "from datetime import datetime\n",
    "from importlib import import_module\n",
    "from itertools import chain\n",
    "from os.path import join, exists\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn.parallel\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from new.aae.pcutil import plot_3d_point_cloud\n",
    "#from utils.util import find_latest_epoch, prepare_results_dir, cuda_setup, setup_logging\n",
    "\n",
    "cudnn.benchmark = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1:\n",
    "        gain = torch.nn.init.calculate_gain('relu')\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0)\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        torch.nn.init.constant_(m.weight, 1)\n",
    "        torch.nn.init.constant_(m.bias, 0)\n",
    "    elif classname.find('Linear') != -1:\n",
    "        gain = torch.nn.init.calculate_gain('relu')\n",
    "        torch.nn.init.xavier_uniform_(m.weight, gain)\n",
    "        if m.bias is not None:\n",
    "            torch.nn.init.constant_(m.bias, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(123)\n",
    "torch.manual_seed(123)\n",
    "torch.cuda.manual_seed_all(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_dir = \"results/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"shapenet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new.shapenet import ShapeNetDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ShapeNetDataset(root_dir=\"shapenet\",\n",
    "                          classes=[\"chair\"])\n",
    "points_dataloader = DataLoader(dataset, batch_size=16,\n",
    "                               shuffle=True,\n",
    "                               num_workers=8,\n",
    "                               drop_last=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if train_langevine:\n",
    "    from new.models import NetWrapper\n",
    "    \n",
    "    net = NetWrapper().to(device)\n",
    "    net.apply(weights_init)\n",
    "\n",
    "else:\n",
    "    from new.aae.aae import Generator, Encoder\n",
    "\n",
    "    G =  Generator().to(device)\n",
    "    E = Encoder().to(device)\n",
    "\n",
    "    G.apply(weights_init)\n",
    "    E.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new.params import *\n",
    "from new.utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Float Tensors\n",
    "#\n",
    "fixed_noise = torch.FloatTensor(16, z_dim)\n",
    "fixed_noise.normal_(mean=0, std=0.2)\n",
    "std_assumed = torch.tensor(0.2)\n",
    "\n",
    "fixed_noise = fixed_noise.to(device)\n",
    "std_assumed = std_assumed.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Optimizers\n",
    "#\n",
    "optim_params = {\n",
    "                \"lr\": 0.0005,\n",
    "                \"weight_decay\": 0,\n",
    "                \"betas\": [0.9, 0.999],\n",
    "                \"amsgrad\": False\n",
    "            }\n",
    "\n",
    "if train_langevine:\n",
    "    optE = torch.optim.Adam(net.netE.parameters(), **optim_params)\n",
    "    optG = torch.optim.Adam(net.netG.parameters(), **optim_params)\n",
    "else:\n",
    "    EG_optim = torch.optim.Adam(chain(E.parameters(), G.parameters()),\n",
    "                        **optim_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new.champfer_loss import ChamferLoss\n",
    "\n",
    "reconstruction_loss = ChamferLoss().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for epoch in range(400):\n",
    "    start_epoch_time = datetime.now()\n",
    "\n",
    "    if train_langevine:\n",
    "        net.train()\n",
    "    else:     \n",
    "        G.train()\n",
    "        E.train()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    for i, point_data in enumerate(points_dataloader, 1):\n",
    "#         if i > 1:\n",
    "#             break\n",
    "\n",
    "        X, _ = point_data\n",
    "        X = X.to(device)\n",
    "\n",
    "        # Change dim [BATCH, N_POINTS, N_DIM] -> [BATCH, N_DIM, N_POINTS]\n",
    "        if X.size(-1) == 3:\n",
    "            X.transpose_(X.dim() - 2, X.dim() - 1)\n",
    "\n",
    "        if train_langevine:\n",
    "            batch_num = X.shape[0]\n",
    "\n",
    "            # Initialize chains\n",
    "            z_g_0 = sample_p_0(n = batch_num,sig=e_init_sig, device = X.device)\n",
    "            z_e_0 = sample_p_0(n = batch_num,sig=g_init_sig, device = X.device)\n",
    "\n",
    "            # Langevin posterior and prior\n",
    "            z_g_k = net(Variable(z_g_0), X, prior=False)\n",
    "            z_e_k = net(Variable(z_e_0), prior=True)\n",
    "\n",
    "            # reconstruction\n",
    "            X_hat = net.netG(z_g_k.detach())\n",
    "            loss_g = net.loss_fun(X_hat.transpose(1,2).contiguous() + 0.5, X.transpose(1,2).contiguous() + 0.5)\n",
    "\n",
    "            # energy prior\n",
    "            en_neg = net.netE(z_e_k.detach()).mean() # TODO(nijkamp): why mean() here and in Langevin sum() over energy? constant is absorbed into Adam adaptive lr\n",
    "            en_pos = net.netE(z_g_k.detach()).mean()\n",
    "            loss_e = en_pos - en_neg\n",
    "\n",
    "            # Learn generator\n",
    "            optG.zero_grad()\n",
    "            loss_g.backward()\n",
    "            optG.step()\n",
    "\n",
    "            optE.zero_grad()\n",
    "            loss_e.backward()\n",
    "            # grad_norm_e = get_grad_norm(net.netE.parameters())\n",
    "            # if args.e_is_grad_clamp:\n",
    "            #    torch.nn.utils.clip_grad_norm_(net.netE.parameters(), args.e_max_norm)\n",
    "            optE.step()\n",
    "\n",
    "\n",
    "            if i % 30 == 0:\n",
    "                print(f'[{epoch}: ({i})] '\n",
    "                          f'loss_g: {loss_g.item():.4f} '\n",
    "                          f'(loss_e: {loss_e.item(): .4f}'\n",
    "                          f' Time: {datetime.now() - start_epoch_time}')\n",
    "\n",
    "\n",
    "        else:\n",
    "            codes, mu, logvar = E(X)\n",
    "            X_rec = G(codes)\n",
    "\n",
    "            loss_e = torch.mean(\n",
    "                 0.05 *\n",
    "                reconstruction_loss(X.permute(0, 2, 1) + 0.5,\n",
    "                                    X_rec.permute(0, 2, 1) + 0.5))\n",
    "\n",
    "            loss_kld = -0.5 * torch.mean(\n",
    "                1 - 2.0 * torch.log(std_assumed) + logvar -\n",
    "                (mu.pow(2) + logvar.exp()) / torch.pow(std_assumed, 2))\n",
    "\n",
    "            loss_eg = loss_e + loss_kld\n",
    "            EG_optim.zero_grad()\n",
    "            E.zero_grad()\n",
    "            G.zero_grad()\n",
    "\n",
    "            loss_eg.backward()\n",
    "            total_loss += loss_eg.item()\n",
    "            EG_optim.step()\n",
    "\n",
    "            if i % 30 == 0:\n",
    "                print(f'[{epoch}: ({i})] '\n",
    "                          f'Loss_EG: {loss_eg.item():.4f} '\n",
    "                          f'(REC: {loss_e.item(): .4f}'\n",
    "                          f' KLD: {loss_kld.item(): .4f})'\n",
    "                          f' Time: {datetime.now() - start_epoch_time}')\n",
    "\n",
    "    print(\n",
    "        f'[{epoch}/{400}] '\n",
    "        f'Loss_G: {total_loss / i:.4f} '\n",
    "        f'Time: {datetime.now() - start_epoch_time}'\n",
    "    )\n",
    "        \n",
    "    ################################### eval ######################################\n",
    "    #\n",
    "    # Save intermediate results\n",
    "    #\n",
    "    if train_langevine:\n",
    "        net.eval()\n",
    "        with torch.no_grad():\n",
    "            fake = net.netG(fixed_noise).data.cpu().numpy()\n",
    "            X_rec = X_hat.data.cpu().numpy()\n",
    "            X = X.data.cpu().numpy()\n",
    "    else:\n",
    "        G.eval()\n",
    "        E.eval()\n",
    "        with torch.no_grad():\n",
    "            fake = G(fixed_noise).data.cpu().numpy()\n",
    "            codes, _, _ = E(X)\n",
    "            X_rec = G(codes).data.cpu().numpy()\n",
    "            X = X.data.cpu().numpy()\n",
    "\n",
    "    for k in range(5):\n",
    "        fig = plot_3d_point_cloud(X[k][0], X[k][1], X[k][2],\n",
    "                                  in_u_sphere=True, show=False)\n",
    "        fig.savefig(\n",
    "            join(results_dir, 'samples', f'{epoch}_{k}_real.png'))\n",
    "        plt.close(fig)\n",
    "\n",
    "    for k in range(5):\n",
    "        fig = plot_3d_point_cloud(fake[k][0], fake[k][1], fake[k][2],\n",
    "                                  in_u_sphere=True, show=False,\n",
    "                                  title=str(epoch))\n",
    "        fig.savefig(\n",
    "            join(results_dir, 'samples', f'{epoch:05}_{k}_fixed.png'))\n",
    "        plt.close(fig)\n",
    "\n",
    "    for k in range(5):\n",
    "        fig = plot_3d_point_cloud(X_rec[k][0],\n",
    "                                  X_rec[k][1],\n",
    "                                  X_rec[k][2],\n",
    "                                  in_u_sphere=True, show=False)\n",
    "        fig.savefig(join(results_dir, 'samples',\n",
    "                         f'{epoch}_{k}_reconstructed.png'))\n",
    "        plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
