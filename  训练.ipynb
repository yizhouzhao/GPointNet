{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_tensorboard = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "if use_tensorboard:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "    writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from utils.util_torch import PointCloudDataSet, PointCloudDataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.10.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2080'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'11.3'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.version.cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new.configs import *\n",
    "from new.utils import * \n",
    "from new.models import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = parse_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if use_tensorboard:\n",
    "    writer.add_hparams(hparam_dict = vars(opt),metric_dict = {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.batch_size = batch_size\n",
    "opt.swap_axis = True\n",
    "if len(opt.checkpoint_path) == 0:\n",
    "    opt.checkpoint_path = None \n",
    "opt.device = \"cuda:%s\" % opt.cuda if opt.cuda!=\"\" else \"cpu\"\n",
    "opt.shuffle = not opt.warm_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(activate_eval=0, activation='ReLU', argment_mode=0, argment_noise=0.01, batch_norm='ln', batch_size=16, beta1_des=0.9, category='chair', checkpoint_path=None, cuda='-1', data_path='data', data_size=10000, debug=99, device='cuda:-1', do_evaluation=1, drop_last=False, eval_step=50, fp16='None', gradient_accumulation_steps=1, langevin_clip=1, langevin_decay=0, learning_mode=0, lr=0.0005, lr_decay=0.998, mode='train', net_type='default_medium', noise_decay=0, normalize='ebp', num_chain=1, num_point=2048, num_steps=2000, output_dir='default', point_dim=3, random_sample=1, ref_sigma=0.3, sample_step=64, seed=666, shuffle=True, stable_check=1, step_size=0.01, swap_axis=True, test_size=16, visualize_mode=0, warm_start=0)\n"
     ]
    }
   ],
   "source": [
    "print(opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = PointCloudDataSet(opt)\n",
    "data_collator = PointCloudDataCollator(opt)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=opt.batch_size, drop_last=opt.drop_last, \n",
    "    shuffle=opt.shuffle, collate_fn = data_collator, num_workers=torch.cuda.device_count() * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'int' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_35356/3274857784.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"data/%s_test.npy\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcategory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m test_loader = torch.utils.data.DataLoader(train_data, batch_size=opt.batch_size, drop_last=opt.drop_last, \n\u001b[1;32m      3\u001b[0m     shuffle=False, collate_fn = data_collator, num_workers=torch.cuda.device_count() * 4)\n",
      "\u001b[0;32m~/anaconda3/envs/ll/lib/python3.8/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *tensors)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Size mismatch between tensors\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ll/lib/python3.8/site-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtensor\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Size mismatch between tensors\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'int' object is not callable"
     ]
    }
   ],
   "source": [
    "# test_dataset = torch.utils.data.TensorDataset(np.load(\"data/%s_test.npy\" % opt.category))\n",
    "# test_loader = torch.utils.data.DataLoader(train_data, batch_size=opt.batch_size, drop_last=opt.drop_last, \n",
    "#     shuffle=False, collate_fn = data_collator, num_workers=torch.cuda.device_count() * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NetWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_gpu(args.device)\n",
    "set_cuda(deterministic=gpu_deterministic)\n",
    "set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optE = torch.optim.Adam(net.netE.parameters(), lr=e_lr, weight_decay=e_decay, betas=(e_beta1, e_beta2))\n",
    "optG = torch.optim.Adam(net.netG.parameters(), lr=g_lr, weight_decay=g_decay, betas=(g_beta1, g_beta2))\n",
    "\n",
    "lr_scheduleE = torch.optim.lr_scheduler.ExponentialLR(optE, e_gamma)\n",
    "lr_scheduleG = torch.optim.lr_scheduler.ExponentialLR(optG, g_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    for c, x in enumerate(train_loader):\n",
    "        total_step += 1\n",
    "        batch_num = x.shape[0]\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.to(\"cuda\")\n",
    "\n",
    "        # Initialize chains\n",
    "        z_g_0 = sample_p_0(n = batch_num,sig=e_init_sig, device = x.device)\n",
    "        z_e_0 = sample_p_0(n = batch_num,sig=e_init_sig, device = x.device)\n",
    "\n",
    "    #     print(\"shape log\")\n",
    "    #     print(x.shape)\n",
    "    #     print(z_g_0.shape)\n",
    "    #     print(z_e_0.shape)\n",
    "\n",
    "        # Langevin posterior and prior\n",
    "        z_g_k = net(Variable(z_g_0), x, prior=False)\n",
    "        z_e_k = net(Variable(z_e_0), prior=True)\n",
    "\n",
    "        # Learn generator\n",
    "        optG.zero_grad()\n",
    "        x_hat = net.netG(z_g_k.detach())\n",
    "        loss_g = net.loss_fun(x_hat.transpose(1,2).contiguous(), x.transpose(1,2).contiguous())\n",
    "        loss_g.backward()\n",
    "        optG.step()\n",
    "\n",
    "        # Learn prior EBM\n",
    "        optE.zero_grad()\n",
    "        en_neg = energy(net.netE(z_e_k.detach())).mean() # TODO(nijkamp): why mean() here and in Langevin sum() over energy? constant is absorbed into Adam adaptive lr\n",
    "        en_pos = energy(net.netE(z_g_k.detach())).mean()\n",
    "        loss_e = en_pos - en_neg\n",
    "        loss_e.backward()\n",
    "        # grad_norm_e = get_grad_norm(net.netE.parameters())\n",
    "        # if args.e_is_grad_clamp:\n",
    "        #    torch.nn.utils.clip_grad_norm_(net.netE.parameters(), args.e_max_norm)\n",
    "        optE.step()\n",
    "\n",
    "        # Printout\n",
    "        if total_step % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                x_0 = net.netG(z_e_0)\n",
    "                x_k = net.netG(z_e_k)\n",
    "\n",
    "                en_neg_2 = energy(net.netE(z_e_k)).mean()\n",
    "                en_pos_2 = energy(net.netE(z_g_k)).mean()\n",
    "\n",
    "                prior_moments = '[{:8.2f}, {:8.2f}, {:8.2f}]'.format(z_e_k.mean(), z_e_k.std(), z_e_k.abs().max())\n",
    "                posterior_moments = '[{:8.2f}, {:8.2f}, {:8.2f}]'.format(z_g_k.mean(), z_g_k.std(), z_g_k.abs().max())\n",
    "                \n",
    "                \n",
    "                if use_tensorboard:\n",
    "                    writer.add_scalar('loss/loss_g',loss_g, total_step)\n",
    "                    writer.add_scalar('loss/loss_e',loss_e, total_step)\n",
    "\n",
    "                    writer.add_scalars('energy/en_pos', {'pos_1':en_pos,\n",
    "                                        'pose_2':en_pos_2,\n",
    "                                        'diff': en_pos_2 - en_pos}, total_step)\n",
    "                    writer.add_scalars('energy/en_neg', {'pos_1':en_neg,\n",
    "                                        'pose_2':en_neg_2,\n",
    "                                        'diff': en_neg_2 - en_neg}, total_step)\n",
    "\n",
    "                    writer.add_scalar('value/|z_g_0|',z_g_0.view(batch_num, -1).norm(dim=1).mean(), total_step)\n",
    "                    writer.add_scalar('value/|z_g_k|',z_g_k.view(batch_num, -1).norm(dim=1).mean(), total_step)\n",
    "                    writer.add_scalar('value/|z_e_0|',z_e_0.view(batch_num, -1).norm(dim=1).mean(), total_step)\n",
    "                    writer.add_scalar('value/|z_e_k|',z_e_k.view(batch_num, -1).norm(dim=1).mean(), total_step)\n",
    "\n",
    "                    writer.add_scalar('disp/z_e_disp',(z_e_k-z_e_0).view(batch_num, -1).norm(dim=1).mean(), total_step)\n",
    "                    writer.add_scalar('disp/z_g_disp',(z_g_k-z_g_0).view(batch_num, -1).norm(dim=1).mean(), total_step)\n",
    "                    writer.add_scalar('disp/x_e_disp',(x_k-x_0).view(batch_num, -1).norm(dim=1).mean(), total_step)\n",
    "\n",
    "                    writer.add_scalars('moment/prior_moments', {'mean':z_e_k.mean(),\n",
    "                                        'std':z_e_k.std(),\n",
    "                                        'max abs': z_e_k.abs().max()}, total_step)\n",
    "                    writer.add_scalars('moment/posterior_moments', {'mean':z_g_k.mean(),\n",
    "                                        'std':z_g_k.std(),\n",
    "                                        'max abs': z_g_k.abs().max()}, total_step)\n",
    "                else:\n",
    "                    print(\n",
    "                        '{} {}/{} {}/{} \\n'.format(0, epoch, n_epochs, total_step, len(train_loader)) +\n",
    "                        'loss_g={:8.5f}, \\n'.format(loss_g) +\n",
    "                        'loss_e={:8.5f}, \\n'.format(loss_e) +\n",
    "                        'en_pos=[{:9.5f}, {:9.5f}, {:9.5f}], \\n'.format(en_pos, en_pos_2, en_pos_2-en_pos) +\n",
    "                        'en_neg=[{:9.5f}, {:9.5f}, {:9.5f}], \\n'.format(en_neg, en_neg_2, en_neg_2-en_neg) +\n",
    "                        '|z_g_0|={:6.3f}, \\n'.format(z_g_0.view(batch_num, -1).norm(dim=1).mean()) +\n",
    "                        '|z_g_k|={:6.3f}, \\n'.format(z_g_k.view(batch_num, -1).norm(dim=1).mean()) +\n",
    "                        '|z_e_0|={:6.3f}, \\n'.format(z_e_0.view(batch_num, -1).norm(dim=1).mean()) +\n",
    "                        '|z_e_k|={:6.3f}, \\n'.format(z_e_k.view(batch_num, -1).norm(dim=1).mean()) +\n",
    "                        'z_e_disp={:6.3f}, \\n'.format((z_e_k-z_e_0).view(batch_num, -1).norm(dim=1).mean()) +\n",
    "                        'z_g_disp={:6.3f}, \\n'.format((z_g_k-z_g_0).view(batch_num, -1).norm(dim=1).mean()) +\n",
    "                        'x_e_disp={:6.3f}, \\n'.format((x_k-x_0).view(batch_num, -1).norm(dim=1).mean()) +\n",
    "                        'prior_moments={}, \\n'.format(prior_moments) +\n",
    "                        'posterior_moments={}, \\n'.format(posterior_moments) +\n",
    "                        #'fid={:8.2f}, '.format(fid) +\n",
    "                        #'fid_best={:8.2f}'.format(fid_best)\n",
    "                        \"\\n\\n\\n ---------------------\"\n",
    "                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.save(net.state_dict(),\"runs/Oct22_12-14-52_yizhou-Z370-AORUS-Gaming-5/net.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
