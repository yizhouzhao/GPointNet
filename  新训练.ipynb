{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from chamferdist import ChamferDistance\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from new.configs import *\n",
    "from new.utils import * \n",
    "from new.models import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = parse_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "writer.add_hparams(hparam_dict = vars(opt),metric_dict = {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.batch_size = batch_size\n",
    "opt.swap_axis = True\n",
    "if len(opt.checkpoint_path) == 0:\n",
    "    opt.checkpoint_path = None \n",
    "opt.device = \"cuda:%s\" % opt.cuda if opt.cuda!=\"\" else \"cpu\"\n",
    "opt.shuffle = not opt.warm_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eval not available.\n"
     ]
    }
   ],
   "source": [
    "from utils.util_torch import PointCloudDataSet, PointCloudDataCollator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = PointCloudDataSet(opt)\n",
    "data_collator = PointCloudDataCollator(opt)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=opt.batch_size, drop_last=opt.drop_last, \n",
    "shuffle=opt.shuffle, collate_fn = data_collator, num_workers=torch.cuda.device_count() * 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 3, 2048])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = NetWrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    net = net.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set_gpu(args.device)\n",
    "set_cuda(deterministic=gpu_deterministic)\n",
    "set_seed(123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optE = torch.optim.Adam(net.netE.parameters(), lr=e_lr, weight_decay=e_decay, betas=(e_beta1, e_beta2))\n",
    "optG = torch.optim.Adam(net.netG.parameters(), lr=g_lr, weight_decay=g_decay, betas=(g_beta1, g_beta2))\n",
    "\n",
    "lr_scheduleE = torch.optim.lr_scheduler.ExponentialLR(optE, e_gamma)\n",
    "lr_scheduleG = torch.optim.lr_scheduler.ExponentialLR(optG, g_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NetWrapper(\n",
       "  (netE): NetE(\n",
       "    (ebm): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=512, bias=True)\n",
       "      (1): GELU()\n",
       "      (2): Linear(in_features=512, out_features=64, bias=True)\n",
       "      (3): GELU()\n",
       "      (4): Linear(in_features=64, out_features=1, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (netG): NetG(\n",
       "    (gen): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=256, bias=True)\n",
       "      (1): GELU()\n",
       "      (2): Linear(in_features=256, out_features=512, bias=True)\n",
       "      (3): GELU()\n",
       "      (4): Linear(in_features=512, out_features=1024, bias=True)\n",
       "      (5): GELU()\n",
       "      (6): Linear(in_features=1024, out_features=6144, bias=True)\n",
       "    )\n",
       "  )\n",
       "  (loss_fun): ChamferDistance()\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(100):\n",
    "    for c, x in enumerate(train_loader):\n",
    "        total_step += 1\n",
    "        batch_num = x.shape[0]\n",
    "\n",
    "        if torch.cuda.is_available():\n",
    "            x = x.to(\"cuda\")\n",
    "\n",
    "        # Initialize chains\n",
    "        z_g_0 = sample_p_0(n = batch_num,sig=e_init_sig, device = x.device)\n",
    "        z_e_0 = sample_p_0(n = batch_num,sig=e_init_sig, device = x.device)\n",
    "\n",
    "    #     print(\"shape log\")\n",
    "    #     print(x.shape)\n",
    "    #     print(z_g_0.shape)\n",
    "    #     print(z_e_0.shape)\n",
    "\n",
    "        # Langevin posterior and prior\n",
    "        z_g_k = net(Variable(z_g_0), x, prior=False)\n",
    "        z_e_k = net(Variable(z_e_0), prior=True)\n",
    "\n",
    "        # Learn generator\n",
    "        optG.zero_grad()\n",
    "        x_hat = net.netG(z_g_k.detach())\n",
    "        loss_g = net.loss_fun(x_hat.transpose(1,2), x.transpose(1,2)) / batch_num\n",
    "        loss_g.backward()\n",
    "        optG.step()\n",
    "\n",
    "        # Learn prior EBM\n",
    "        optE.zero_grad()\n",
    "        en_neg = energy(net.netE(z_e_k.detach())).mean() # TODO(nijkamp): why mean() here and in Langevin sum() over energy? constant is absorbed into Adam adaptive lr\n",
    "        en_pos = energy(net.netE(z_g_k.detach())).mean()\n",
    "        loss_e = en_pos - en_neg\n",
    "        loss_e.backward()\n",
    "        # grad_norm_e = get_grad_norm(net.netE.parameters())\n",
    "        # if args.e_is_grad_clamp:\n",
    "        #    torch.nn.utils.clip_grad_norm_(net.netE.parameters(), args.e_max_norm)\n",
    "        optE.step()\n",
    "\n",
    "        # Printout\n",
    "        if total_step % 10 == 0:\n",
    "            with torch.no_grad():\n",
    "                x_0 = net.netG(z_e_0)\n",
    "                x_k = net.netG(z_e_k)\n",
    "\n",
    "                en_neg_2 = energy(net.netE(z_e_k)).mean()\n",
    "                en_pos_2 = energy(net.netE(z_g_k)).mean()\n",
    "\n",
    "                prior_moments = '[{:8.2f}, {:8.2f}, {:8.2f}]'.format(z_e_k.mean(), z_e_k.std(), z_e_k.abs().max())\n",
    "                posterior_moments = '[{:8.2f}, {:8.2f}, {:8.2f}]'.format(z_g_k.mean(), z_g_k.std(), z_g_k.abs().max())\n",
    "                \n",
    "                writer.add_scalar('loss/loss_g',loss_g, total_step)\n",
    "                writer.add_scalar('loss/loss_e',loss_e, total_step)\n",
    "                \n",
    "                writer.add_scalars('energy/en_pos', {'pos_1':en_pos,\n",
    "                                    'pose_2':en_pos_2,\n",
    "                                    'diff': en_pos_2 - en_pos}, total_step)\n",
    "                writer.add_scalars('energy/en_neg', {'pos_1':en_neg,\n",
    "                                    'pose_2':en_neg_2,\n",
    "                                    'diff': en_neg_2 - en_neg}, total_step)\n",
    "                \n",
    "                writer.add_scalar('value/|z_g_0|',z_g_0.view(batch_num, -1).norm(dim=1).mean(), total_step)\n",
    "                writer.add_scalar('value/|z_g_k|',z_g_k.view(batch_num, -1).norm(dim=1).mean(), total_step)\n",
    "                writer.add_scalar('value/|z_e_0|',z_e_0.view(batch_num, -1).norm(dim=1).mean(), total_step)\n",
    "                writer.add_scalar('value/|z_e_k|',z_e_k.view(batch_num, -1).norm(dim=1).mean(), total_step)\n",
    "                \n",
    "                writer.add_scalar('disp/z_e_disp',(z_e_k-z_e_0).view(batch_num, -1).norm(dim=1).mean(), total_step)\n",
    "                writer.add_scalar('disp/z_g_disp',(z_g_k-z_g_0).view(batch_num, -1).norm(dim=1).mean(), total_step)\n",
    "                writer.add_scalar('disp/x_e_disp',(x_k-x_0).view(batch_num, -1).norm(dim=1).mean(), total_step)\n",
    "                \n",
    "                writer.add_scalars('moment/prior_moments', {'mean':z_e_k.mean(),\n",
    "                                    'std':z_e_k.std(),\n",
    "                                    'max abs': z_e_k.abs().max()}, total_step)\n",
    "                writer.add_scalars('moment/posterior_moments', {'mean':z_g_k.mean(),\n",
    "                                    'std':z_g_k.std(),\n",
    "                                    'max abs': z_g_k.abs().max()}, total_step)\n",
    "                \n",
    "                \n",
    "#                 print(\n",
    "#                     '{} {}/{} {}/{} \\n'.format(0, epoch, n_epochs, total_step, len(train_loader)) +\n",
    "#                     'loss_g={:8.3f}, \\n'.format(loss_g) +\n",
    "#                     'loss_e={:8.3f}, \\n'.format(loss_e) +\n",
    "#                     'en_pos=[{:9.4f}, {:9.4f}, {:9.4f}], \\n'.format(en_pos, en_pos_2, en_pos_2-en_pos) +\n",
    "#                     'en_neg=[{:9.4f}, {:9.4f}, {:9.4f}], \\n'.format(en_neg, en_neg_2, en_neg_2-en_neg) +\n",
    "#                     '|z_g_0|={:6.2f}, \\n'.format(z_g_0.view(batch_num, -1).norm(dim=1).mean()) +\n",
    "#                     '|z_g_k|={:6.2f}, \\n'.format(z_g_k.view(batch_num, -1).norm(dim=1).mean()) +\n",
    "#                     '|z_e_0|={:6.2f}, \\n'.format(z_e_0.view(batch_num, -1).norm(dim=1).mean()) +\n",
    "#                     '|z_e_k|={:6.2f}, \\n'.format(z_e_k.view(batch_num, -1).norm(dim=1).mean()) +\n",
    "#                     'z_e_disp={:6.2f}, \\n'.format((z_e_k-z_e_0).view(batch_num, -1).norm(dim=1).mean()) +\n",
    "#                     'z_g_disp={:6.2f}, \\n'.format((z_g_k-z_g_0).view(batch_num, -1).norm(dim=1).mean()) +\n",
    "#                     'x_e_disp={:6.2f}, \\n'.format((x_k-x_0).view(batch_num, -1).norm(dim=1).mean()) +\n",
    "#                     'prior_moments={}, \\n'.format(prior_moments) +\n",
    "#                     'posterior_moments={}, \\n'.format(posterior_moments) +\n",
    "#                     #'fid={:8.2f}, '.format(fid) +\n",
    "#                     #'fid_best={:8.2f}'.format(fid_best)\n",
    "#                     \"\\n\\n\\n ---------------------\"\n",
    "#                 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'add_hparams' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-0fce7c5c3245>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0madd_hparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'add_hparams' is not defined"
     ]
    }
   ],
   "source": [
    "add_hparams"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
